{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-PEFT-PPM Replication Study - Setup and Environment\n",
    "\n",
    "## Setup Instructions for TU/e HPC Jupyter Environment\n",
    "\n",
    "This notebook handles the initial setup and environment configuration for replicating the LLM-PEFT-PPM experiments.\n",
    "\n",
    "### Prerequisites\n",
    "- Jupyter launched with GPU access on TU/e HPC\n",
    "- HuggingFace account and token\n",
    "- Access to the original repository: https://github.com/raseidi/llm-peft-ppm\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Check and System Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import subprocess\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== System Information ===\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Check if running on SLURM\n",
    "if 'SLURM_JOB_ID' in os.environ:\n",
    "    print(f\"\\n=== SLURM Information ===\")\n",
    "    print(f\"Job ID: {os.environ['SLURM_JOB_ID']}\")\n",
    "    print(f\"Node: {os.environ.get('SLURMD_NODENAME', 'Unknown')}\")\n",
    "    print(f\"Partition: {os.environ.get('SLURM_JOB_PARTITION', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Repository Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define project structure\n",
    "PROJECT_ROOT = Path.cwd() / \"llm-peft-ppm-replication\"\n",
    "ORIGINAL_REPO = \"https://github.com/raseidi/llm-peft-ppm.git\"\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "\n",
    "# Clone repository if it doesn't exist\n",
    "if not PROJECT_ROOT.exists():\n",
    "    print(\"Cloning original repository...\")\n",
    "    subprocess.run([\"git\", \"clone\", ORIGINAL_REPO, str(PROJECT_ROOT)], check=True)\n",
    "    print(\"Repository cloned successfully!\")\n",
    "else:\n",
    "    print(\"Repository already exists\")\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"Changed to directory: {os.getcwd()}\")\n",
    "\n",
    "# List repository contents\n",
    "print(\"\\nRepository structure:\")\n",
    "for item in sorted(PROJECT_ROOT.glob(\"*\")):\n",
    "    print(f\"  {'üìÅ' if item.is_dir() else 'üìÑ'} {item.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dependencies Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv package manager if not available\n",
    "try:\n",
    "    subprocess.run([\"uv\", \"--version\"], check=True, capture_output=True)\n",
    "    print(\"uv is already installed\")\n",
    "except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "    print(\"Installing uv...\")\n",
    "    subprocess.run([\n",
    "        \"curl\", \"-LsSf\", \"https://astral.sh/uv/install.sh\"\n",
    "    ], shell=True, check=True)\n",
    "    \n",
    "    # Add uv to PATH for current session\n",
    "    uv_path = Path.home() / \".local\" / \"bin\"\n",
    "    if str(uv_path) not in os.environ[\"PATH\"]:\n",
    "        os.environ[\"PATH\"] = f\"{uv_path}:{os.environ['PATH']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create virtual environment and install dependencies\n",
    "venv_path = PROJECT_ROOT / \".venv\"\n",
    "\n",
    "if not venv_path.exists():\n",
    "    print(\"Creating virtual environment...\")\n",
    "    subprocess.run([\"uv\", \"venv\", \".venv\", \"--python\", \"3.12\"], check=True)\n",
    "    print(\"Virtual environment created!\")\n",
    "\n",
    "# Install requirements\n",
    "print(\"Installing dependencies...\")\n",
    "subprocess.run([\"uv\", \"pip\", \"install\", \"-r\", \"requirements.txt\"], check=True)\n",
    "\n",
    "# Install additional packages mentioned in methodology\n",
    "additional_packages = [\n",
    "    \"pyyaml\",\n",
    "    \"jupyter\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"plotly\",\n",
    "    \"tqdm\",\n",
    "    \"scikit-learn\"\n",
    "]\n",
    "\n",
    "for package in additional_packages:\n",
    "    try:\n",
    "        subprocess.run([\"uv\", \"pip\", \"install\", package], check=True)\n",
    "        print(f\"‚úÖ Installed {package}\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"‚ùå Failed to install {package}\")\n",
    "\n",
    "print(\"\\nDependency installation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HuggingFace Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace token setup\n",
    "import getpass\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if HF token is already configured\n",
    "env_file = PROJECT_ROOT / \".env\"\n",
    "\n",
    "if not env_file.exists() and \"HF_TOKEN\" not in os.environ:\n",
    "    print(\"HuggingFace token not found. Please provide your token:\")\n",
    "    print(\"You can get a token from: https://huggingface.co/docs/hub/en/security-tokens\")\n",
    "    \n",
    "    hf_token = getpass.getpass(\"Enter your HuggingFace token: \")\n",
    "    \n",
    "    # Save to .env file\n",
    "    with open(env_file, \"w\") as f:\n",
    "        f.write(f\"HF_TOKEN={hf_token}\\n\")\n",
    "    \n",
    "    print(\"‚úÖ HuggingFace token saved to .env file\")\n",
    "    \n",
    "    # Set environment variable for current session\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token\n",
    "else:\n",
    "    print(\"‚úÖ HuggingFace token already configured\")\n",
    "    \n",
    "    # Load from .env if exists\n",
    "    if env_file.exists() and \"HF_TOKEN\" not in os.environ:\n",
    "        with open(env_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if line.startswith(\"HF_TOKEN=\"):\n",
    "                    os.environ[\"HF_TOKEN\"] = line.split(\"=\", 1)[1].strip()\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test HuggingFace connection\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    \n",
    "    # Test with a small public model\n",
    "    print(\"Testing HuggingFace connection...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "    print(\"‚úÖ HuggingFace connection successful!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå HuggingFace connection failed: {e}\")\n",
    "    print(\"Please check your token and internet connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Project Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional directories for replication study\n",
    "directories = [\n",
    "    \"replication_results\",\n",
    "    \"replication_results/experiments\",\n",
    "    \"replication_results/logs\",\n",
    "    \"replication_results/models\",\n",
    "    \"replication_results/plots\",\n",
    "    \"replication_notebooks\",\n",
    "    \"data_analysis\",\n",
    "    \"traffic_fines_data\"  # For the additional dataset\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    dir_path = PROJECT_ROOT / directory\n",
    "    dir_path.mkdir(exist_ok=True)\n",
    "    print(f\"üìÅ Created/verified: {directory}\")\n",
    "\n",
    "print(\"\\n‚úÖ Project structure setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quick Test of Main Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test import of main script\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "try:\n",
    "    # Test if we can import the main modules\n",
    "    print(\"Testing imports...\")\n",
    "    \n",
    "    # Import main training script components\n",
    "    exec(open(\"next_event_prediction.py\").read().split(\"if __name__\")[0])\n",
    "    print(\"‚úÖ Main script imports successful\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Import test failed: {e}\")\n",
    "    print(\"This is normal if dependencies aren't fully compatible yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dataset Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if datasets exist or need to be downloaded\n",
    "data_dir = PROJECT_ROOT / \"data\"\n",
    "\n",
    "expected_datasets = [\n",
    "    \"BPI12\",\n",
    "    \"BPI17\", \n",
    "    \"BPI20RfP\",\n",
    "    \"BPI20PTC\",\n",
    "    \"BPI20PD\", \n",
    "    \"BPI_Traffic_Fines\",\n",
    "]\n",
    "\n",
    "print(\"Checking dataset availability...\")\n",
    "for dataset in expected_datasets:\n",
    "    dataset_path = data_dir / dataset\n",
    "    if dataset_path.exists():\n",
    "        print(f\"‚úÖ {dataset}: Found\")\n",
    "    else:\n",
    "        print(f\"üì• {dataset}: Will be downloaded automatically on first run\")\n",
    "\n",
    "print(\"\\nNote: Datasets will be automatically downloaded via SkPM when experiments run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Configuration Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration summary\n",
    "config_summary = {\n",
    "    \"environment\": {\n",
    "        \"python_version\": sys.version,\n",
    "        \"pytorch_version\": torch.__version__,\n",
    "        \"cuda_available\": torch.cuda.is_available(),\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\",\n",
    "        \"project_root\": str(PROJECT_ROOT)\n",
    "    },\n",
    "    \"datasets\": expected_datasets,\n",
    "    \"experiment_parameters\": {\n",
    "        \"models\": [\"qwen25-05b\", \"llama32-1b\", \"pm-gpt2\"],\n",
    "        \"peft_techniques\": [\"lora\", \"layer_freezing\"],\n",
    "        \"baseline\": \"LSTM\",\n",
    "        \"epochs_llm\": 10,\n",
    "        \"epochs_rnn\": 25,\n",
    "        \"lora_r\": 256,\n",
    "        \"lora_alpha\": 512\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "import json\n",
    "config_file = PROJECT_ROOT / \"replication_results\" / \"experiment_config.json\"\n",
    "with open(config_file, \"w\") as f:\n",
    "    json.dump(config_summary, f, indent=2, default=str)\n",
    "\n",
    "print(\"=== Experiment Configuration ===\")\n",
    "print(json.dumps(config_summary, indent=2, default=str))\n",
    "print(f\"\\n Configuration saved to: {config_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "**Setup Complete!** \n",
    "<!-- \n",
    "### Ready to proceed with:\n",
    "\n",
    "1. **Data Exploration** (`02_data_exploration.ipynb`)\n",
    "   - Analyze the 5 original datasets\n",
    "   - Prepare Traffic Fines dataset\n",
    "   - Preprocessing and feature engineering\n",
    "\n",
    "2. **Baseline Experiments** (`03_baseline_experiments.ipynb`)\n",
    "   - RNN baseline with hyperparameter search\n",
    "   - Traditional approaches comparison\n",
    "\n",
    "3. **LLM-PEFT Experiments** (`04_llm_peft_experiments.ipynb`)\n",
    "   - LoRA fine-tuning experiments\n",
    "   - Layer freezing strategies\n",
    "   - Multi-task vs single-task learning\n",
    "\n",
    "4. **Results Analysis** (`05_results_analysis.ipynb`)\n",
    "   - Performance comparison\n",
    "   - Statistical significance testing\n",
    "   - Visualization of results\n",
    "\n",
    "5. **Competitor Baselines** (`06_competitor_baselines.ipynb`)\n",
    "   - S-NAP narrative approach\n",
    "   - Transfer learning baseline\n",
    "\n",
    " -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
