{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-PEFT-PPM Results Analysis\n",
    "\n",
    "- Performance metric extraction and comparison\n",
    "- Statistical significance testing\n",
    "- Comparison with original paper results\n",
    "- Visualization of findings\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Project setup\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if not (PROJECT_ROOT / \"replication_results\").exists():\n",
    "    PROJECT_ROOT = Path.cwd() / \"llm-peft-ppm-replication\"\n",
    "    os.chdir(PROJECT_ROOT)\n",
    "\n",
    "RESULTS_DIR = PROJECT_ROOT / \"replication_results\"\n",
    "PLOTS_DIR = RESULTS_DIR / \"plots\"\n",
    "PLOTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Working directory: {PROJECT_ROOT}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"Plots directory: {PLOTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment results\n",
    "def load_results(filename):\n",
    "    filepath = RESULTS_DIR / filename\n",
    "    if filepath.exists():\n",
    "        with open(filepath, 'r') as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        print(f\" File not found: {filename}\")\n",
    "        return {}\n",
    "\n",
    "# Load all result files\n",
    "rnn_results = load_results(\"rnn_baseline_results.json\")\n",
    "llm_results = load_results(\"llm_peft_results.json\")\n",
    "competitor_results = load_results(\"competitor_baseline_results.json\")\n",
    "\n",
    "print(f\"Loaded results:\")\n",
    "print(f\"  - RNN experiments: {len(rnn_results)} datasets\")\n",
    "print(f\"  - LLM experiments: {len(llm_results)} datasets\")\n",
    "print(f\"  - Competitor experiments: {len(competitor_results)} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performance Metrics Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metrics_from_log(log_file_path):\n",
    "    \"\"\"\n",
    "    Extract performance metrics from experiment log files.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    if not Path(log_file_path).exists():\n",
    "        return metrics\n",
    "    \n",
    "    try:\n",
    "        with open(log_file_path, 'r') as f:\n",
    "            log_content = f.read()\n",
    "        \n",
    "        # Extract accuracy for next activity prediction\n",
    "        accuracy_pattern = r\"Test accuracy[:\\s]+([0-9.]+)\"\n",
    "        accuracy_matches = re.findall(accuracy_pattern, log_content, re.IGNORECASE)\n",
    "        if accuracy_matches:\n",
    "            metrics['next_activity_accuracy'] = float(accuracy_matches[-1])  # Take last/best\n",
    "        \n",
    "        # Extract MSE for remaining time prediction\n",
    "        mse_pattern = r\"Test MSE[:\\s]+([0-9.]+)\"\n",
    "        mse_matches = re.findall(mse_pattern, log_content, re.IGNORECASE)\n",
    "        if mse_matches:\n",
    "            metrics['remaining_time_mse'] = float(mse_matches[-1])  # Take last/best\n",
    "        \n",
    "        # Extract training time\n",
    "        time_pattern = r\"Training time[:\\s]+([0-9.]+)\\s*(?:seconds|s)\"\n",
    "        time_matches = re.findall(time_pattern, log_content, re.IGNORECASE)\n",
    "        if time_matches:\n",
    "            metrics['training_time'] = float(time_matches[-1])\n",
    "        \n",
    "        # Extract epoch information\n",
    "        epoch_pattern = r\"Epoch\\s+([0-9]+)\"\n",
    "        epoch_matches = re.findall(epoch_pattern, log_content)\n",
    "        if epoch_matches:\n",
    "            metrics['epochs_completed'] = max([int(e) for e in epoch_matches])\n",
    "        \n",
    "        # Look for final evaluation metrics\n",
    "        final_pattern = r\"Final\\s+(?:test\\s+)?(?:results?[:\\s]+)?.*accuracy[:\\s]+([0-9.]+).*mse[:\\s]+([0-9.]+)\"\n",
    "        final_matches = re.findall(final_pattern, log_content, re.IGNORECASE | re.DOTALL)\n",
    "        if final_matches:\n",
    "            metrics['next_activity_accuracy'] = float(final_matches[-1][0])\n",
    "            metrics['remaining_time_mse'] = float(final_matches[-1][1])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting metrics from {log_file_path}: {e}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def build_results_dataframe(all_results):\n",
    "    \"\"\"\n",
    "    Build a comprehensive DataFrame from all experiment results.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for exp_type, results in all_results.items():\n",
    "        for dataset_name, dataset_results in results.items():\n",
    "            if isinstance(dataset_results, dict):\n",
    "                for config_name, config_results in dataset_results.items():\n",
    "                    if isinstance(config_results, dict):\n",
    "                        if \"status\" in config_results:  # Direct result\n",
    "                            row = {\n",
    "                                'experiment_type': exp_type,\n",
    "                                'dataset': dataset_name,\n",
    "                                'model': 'RNN' if exp_type == 'RNN' else config_name,\n",
    "                                'configuration': config_name,\n",
    "                                'status': config_results['status'],\n",
    "                                'runtime': config_results.get('runtime', 0)\n",
    "                            }\n",
    "                            \n",
    "                            # Extract metrics from log file if available\n",
    "                            if 'log_file' in config_results:\n",
    "                                metrics = extract_metrics_from_log(config_results['log_file'])\n",
    "                                row.update(metrics)\n",
    "                            \n",
    "                            rows.append(row)\n",
    "                        \n",
    "                        else:  # Nested results (LLM experiments)\n",
    "                            for strategy_name, strategy_results in config_results.items():\n",
    "                                if isinstance(strategy_results, dict) and \"status\" in strategy_results:\n",
    "                                    row = {\n",
    "                                        'experiment_type': exp_type,\n",
    "                                        'dataset': dataset_name,\n",
    "                                        'model': config_name,\n",
    "                                        'peft_strategy': strategy_name,\n",
    "                                        'configuration': f\"{config_name}_{strategy_name}\",\n",
    "                                        'status': strategy_results['status'],\n",
    "                                        'runtime': strategy_results.get('runtime', 0)\n",
    "                                    }\n",
    "                                    \n",
    "                                    # Extract metrics from log file if available\n",
    "                                    if 'log_file' in strategy_results:\n",
    "                                        metrics = extract_metrics_from_log(strategy_results['log_file'])\n",
    "                                        row.update(metrics)\n",
    "                                    \n",
    "                                    rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Build comprehensive results DataFrame\n",
    "all_results = {\n",
    "    'RNN': rnn_results,\n",
    "    'LLM-PEFT': llm_results,\n",
    "    'Competitor': competitor_results\n",
    "}\n",
    "\n",
    "results_df = build_results_dataframe(all_results)\n",
    "\n",
    "print(f\"\\n Results DataFrame created with {len(results_df)} experiments\")\n",
    "print(f\"Columns: {list(results_df.columns)}\")\n",
    "print(f\"\\nExperiment status distribution:\")\n",
    "print(results_df['status'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of results\n",
    "if not results_df.empty:\n",
    "    print(\"\\n Sample Results:\")\n",
    "    print(results_df.head(10))\n",
    "    \n",
    "    # Show successful experiments only\n",
    "    successful_df = results_df[results_df['status'] == 'success']\n",
    "    print(f\"\\n Successful experiments: {len(successful_df)}/{len(results_df)}\")\n",
    "    \n",
    "    if not successful_df.empty:\n",
    "        print(\"\\nMetrics available for successful experiments:\")\n",
    "        metric_columns = ['next_activity_accuracy', 'remaining_time_mse', 'training_time', 'epochs_completed']\n",
    "        available_metrics = [col for col in metric_columns if col in successful_df.columns and successful_df[col].notna().any()]\n",
    "        print(f\"Available metrics: {available_metrics}\")\n",
    "        \n",
    "        if available_metrics:\n",
    "            print(\"\\n📈 Sample metrics:\")\n",
    "            sample_metrics = successful_df[['dataset', 'model', 'configuration'] + available_metrics].head(5)\n",
    "            print(sample_metrics)\n",
    "else:\n",
    "    print(\" No results found. Please run experiments first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Comparison Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance comparison tables\n",
    "if not results_df.empty and len(results_df[results_df['status'] == 'success']) > 0:\n",
    "    \n",
    "    successful_df = results_df[results_df['status'] == 'success'].copy()\n",
    "    \n",
    "    # Next Activity Prediction Performance\n",
    "    if 'next_activity_accuracy' in successful_df.columns:\n",
    "        print(\"\\n Next Activity Prediction Performance\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Best performance by dataset and model type\n",
    "        na_performance = successful_df.groupby(['dataset', 'experiment_type'])['next_activity_accuracy'].agg(['max', 'mean', 'std']).round(4)\n",
    "        print(na_performance)\n",
    "        \n",
    "        # Save to CSV\n",
    "        na_performance.to_csv(RESULTS_DIR / \"next_activity_performance.csv\")\n",
    "    \n",
    "    # Remaining Time Prediction Performance\n",
    "    if 'remaining_time_mse' in successful_df.columns:\n",
    "        print(\"\\n Remaining Time Prediction Performance (MSE - lower is better)\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Best performance by dataset and model type\n",
    "        rt_performance = successful_df.groupby(['dataset', 'experiment_type'])['remaining_time_mse'].agg(['min', 'mean', 'std']).round(4)\n",
    "        print(rt_performance)\n",
    "        \n",
    "        # Save to CSV\n",
    "        rt_performance.to_csv(RESULTS_DIR / \"remaining_time_performance.csv\")\n",
    "    \n",
    "    # Best performing configurations per dataset\n",
    "    print(\"\\n Best Performing Configurations per Dataset\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for dataset in successful_df['dataset'].unique():\n",
    "        dataset_df = successful_df[successful_df['dataset'] == dataset]\n",
    "        \n",
    "        print(f\"\\n {dataset}:\")\n",
    "        \n",
    "        if 'next_activity_accuracy' in dataset_df.columns:\n",
    "            best_na = dataset_df.loc[dataset_df['next_activity_accuracy'].idxmax()]\n",
    "            print(f\"  Best Next Activity: {best_na['configuration']} (Acc: {best_na['next_activity_accuracy']:.4f})\")\n",
    "        \n",
    "        if 'remaining_time_mse' in dataset_df.columns:\n",
    "            best_rt = dataset_df.loc[dataset_df['remaining_time_mse'].idxmin()]\n",
    "            print(f\"  Best Remaining Time: {best_rt['configuration']} (MSE: {best_rt['remaining_time_mse']:.4f})\")\n",
    "    \n",
    "    # Save detailed results\n",
    "    successful_df.to_csv(RESULTS_DIR / \"detailed_results.csv\", index=False)\n",
    "    print(f\"\\n Detailed results saved to: {RESULTS_DIR / 'detailed_results.csv'}\")\n",
    "\n",
    "else:\n",
    "    print(\" No successful experiments found for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical significance testing\n",
    "if not results_df.empty and len(results_df[results_df['status'] == 'success']) > 0:\n",
    "    \n",
    "    successful_df = results_df[results_df['status'] == 'success'].copy()\n",
    "    \n",
    "    print(\"\\n Statistical Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Compare RNN vs LLM-PEFT performance\n",
    "    if 'next_activity_accuracy' in successful_df.columns:\n",
    "        rnn_scores = successful_df[successful_df['experiment_type'] == 'RNN']['next_activity_accuracy'].dropna()\n",
    "        llm_scores = successful_df[successful_df['experiment_type'] == 'LLM-PEFT']['next_activity_accuracy'].dropna()\n",
    "        \n",
    "        if len(rnn_scores) > 1 and len(llm_scores) > 1:\n",
    "            # Perform t-test\n",
    "            t_stat, p_value = stats.ttest_ind(llm_scores, rnn_scores)\n",
    "            \n",
    "            print(f\"\\n🔬 Next Activity Prediction: LLM-PEFT vs RNN\")\n",
    "            print(f\"  RNN Mean Accuracy: {rnn_scores.mean():.4f} ± {rnn_scores.std():.4f} (n={len(rnn_scores)})\")\n",
    "            print(f\"  LLM Mean Accuracy: {llm_scores.mean():.4f} ± {llm_scores.std():.4f} (n={len(llm_scores)})\")\n",
    "            print(f\"  T-statistic: {t_stat:.4f}\")\n",
    "            print(f\"  P-value: {p_value:.4f}\")\n",
    "            print(f\"  Significant difference: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "    \n",
    "    # PEFT strategy comparison\n",
    "    if 'peft_strategy' in successful_df.columns and 'next_activity_accuracy' in successful_df.columns:\n",
    "        llm_df = successful_df[successful_df['experiment_type'] == 'LLM-PEFT']\n",
    "        \n",
    "        print(f\"\\n PEFT Strategy Comparison\")\n",
    "        strategy_performance = llm_df.groupby('peft_strategy')['next_activity_accuracy'].agg(['count', 'mean', 'std']).round(4)\n",
    "        print(strategy_performance)\n",
    "        \n",
    "        # ANOVA test for strategy differences\n",
    "        strategies = llm_df['peft_strategy'].unique()\n",
    "        if len(strategies) > 2:\n",
    "            strategy_groups = [llm_df[llm_df['peft_strategy'] == s]['next_activity_accuracy'].dropna() for s in strategies]\n",
    "            strategy_groups = [g for g in strategy_groups if len(g) > 0]\n",
    "            \n",
    "            if len(strategy_groups) > 1:\n",
    "                f_stat, p_value = stats.f_oneway(*strategy_groups)\n",
    "                print(f\"\\n  ANOVA F-statistic: {f_stat:.4f}\")\n",
    "                print(f\"  P-value: {p_value:.4f}\")\n",
    "                print(f\"  Significant strategy differences: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "    \n",
    "    # Multi-task vs Single-task analysis (if data available)\n",
    "    # Note: This would require additional information about task configuration\n",
    "    \n",
    "    # Training efficiency analysis\n",
    "    if 'training_time' in successful_df.columns:\n",
    "        print(f\"\\n Training Efficiency Analysis\")\n",
    "        efficiency = successful_df.groupby('experiment_type')['training_time'].agg(['count', 'mean', 'std']).round(2)\n",
    "        print(efficiency)\n",
    "        \n",
    "        # Training time comparison\n",
    "        rnn_times = successful_df[successful_df['experiment_type'] == 'RNN']['training_time'].dropna()\n",
    "        llm_times = successful_df[successful_df['experiment_type'] == 'LLM-PEFT']['training_time'].dropna()\n",
    "        \n",
    "        if len(rnn_times) > 1 and len(llm_times) > 1:\n",
    "            t_stat, p_value = stats.ttest_ind(llm_times, rnn_times)\n",
    "            print(f\"\\n  Training Time Comparison (seconds):\")\n",
    "            print(f\"    RNN: {rnn_times.mean():.1f} ± {rnn_times.std():.1f}\")\n",
    "            print(f\"    LLM-PEFT: {llm_times.mean():.1f} ± {llm_times.std():.1f}\")\n",
    "            print(f\"    P-value: {p_value:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\" Insufficient data for statistical analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "if not results_df.empty and len(results_df[results_df['status'] == 'success']) > 0:\n",
    "    \n",
    "    successful_df = results_df[results_df['status'] == 'success'].copy()\n",
    "    \n",
    "    # Performance comparison plots\n",
    "    if 'next_activity_accuracy' in successful_df.columns:\n",
    "        \n",
    "        # 1. Box plot comparison by experiment type\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.boxplot(data=successful_df, x='dataset', y='next_activity_accuracy', hue='experiment_type')\n",
    "        plt.title('Next Activity Prediction Accuracy by Dataset and Method')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(PLOTS_DIR / 'accuracy_comparison_boxplot.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # 2. PEFT strategy comparison\n",
    "        if 'peft_strategy' in successful_df.columns:\n",
    "            llm_df = successful_df[successful_df['experiment_type'] == 'LLM-PEFT']\n",
    "            if not llm_df.empty:\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                sns.barplot(data=llm_df, x='peft_strategy', y='next_activity_accuracy', ci=95)\n",
    "                plt.title('PEFT Strategy Performance Comparison')\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.ylabel('Next Activity Accuracy')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(PLOTS_DIR / 'peft_strategy_comparison.png', dpi=300, bbox_inches='tight')\n",
    "                plt.show()\n",
    "    \n",
    "    # Training time comparison\n",
    "    if 'training_time' in successful_df.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(data=successful_df, x='experiment_type', y='training_time')\n",
    "        plt.title('Training Time Comparison')\n",
    "        plt.ylabel('Training Time (seconds)')\n",
    "        plt.yscale('log')  # Log scale for better visualization\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(PLOTS_DIR / 'training_time_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # Performance vs runtime scatter plot\n",
    "    if 'next_activity_accuracy' in successful_df.columns and 'runtime' in successful_df.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for exp_type in successful_df['experiment_type'].unique():\n",
    "            type_df = successful_df[successful_df['experiment_type'] == exp_type]\n",
    "            plt.scatter(type_df['runtime'], type_df['next_activity_accuracy'], \n",
    "                       label=exp_type, alpha=0.7, s=60)\n",
    "        \n",
    "        plt.xlabel('Runtime (seconds)')\n",
    "        plt.ylabel('Next Activity Accuracy')\n",
    "        plt.title('Performance vs Runtime')\n",
    "        plt.legend()\n",
    "        plt.xscale('log')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(PLOTS_DIR / 'performance_vs_runtime.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # Dataset performance heatmap\n",
    "    if 'next_activity_accuracy' in successful_df.columns:\n",
    "        # Create pivot table for heatmap\n",
    "        pivot_data = successful_df.pivot_table(\n",
    "            values='next_activity_accuracy', \n",
    "            index='dataset', \n",
    "            columns='experiment_type', \n",
    "            aggfunc='max'\n",
    "        )\n",
    "        \n",
    "        if not pivot_data.empty:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='viridis')\n",
    "            plt.title('Best Accuracy by Dataset and Method')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(PLOTS_DIR / 'performance_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "    \n",
    "    print(f\"\\n Plots saved to: {PLOTS_DIR}\")\n",
    "\n",
    "else:\n",
    "    print(\" No successful experiments found for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison with Original Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original paper results (manually entered based on paper)\n",
    "# Note: These would need to be updated with actual values from the paper\n",
    "\n",
    "original_results = {\n",
    "    \"BPI12\": {\n",
    "        \"RNN_baseline\": {\"accuracy\": 0.85, \"mse\": 2.1},  # Example values\n",
    "        \"LLM_PEFT_best\": {\"accuracy\": 0.92, \"mse\": 1.8},\n",
    "        \"S_NAP\": {\"accuracy\": 0.88, \"mse\": 2.0}\n",
    "    },\n",
    "    \"BPI17\": {\n",
    "        \"RNN_baseline\": {\"accuracy\": 0.82, \"mse\": 3.2},\n",
    "        \"LLM_PEFT_best\": {\"accuracy\": 0.89, \"mse\": 2.9},\n",
    "        \"S_NAP\": {\"accuracy\": 0.85, \"mse\": 3.1}\n",
    "    }\n",
    "    # Add more datasets as needed\n",
    "}\n",
    "\n",
    "print(\"\\n Comparison with Original Paper Results\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Note: Original paper values are examples and should be updated with actual results.\")\n",
    "\n",
    "if not results_df.empty and len(results_df[results_df['status'] == 'success']) > 0:\n",
    "    successful_df = results_df[results_df['status'] == 'success'].copy()\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for dataset in original_results.keys():\n",
    "        if dataset in successful_df['dataset'].values:\n",
    "            dataset_df = successful_df[successful_df['dataset'] == dataset]\n",
    "            \n",
    "            # Get our best results\n",
    "            if 'next_activity_accuracy' in dataset_df.columns:\n",
    "                rnn_results_ours = dataset_df[dataset_df['experiment_type'] == 'RNN']['next_activity_accuracy']\n",
    "                llm_results_ours = dataset_df[dataset_df['experiment_type'] == 'LLM-PEFT']['next_activity_accuracy']\n",
    "                \n",
    "                our_best_rnn = rnn_results_ours.max() if not rnn_results_ours.empty else None\n",
    "                our_best_llm = llm_results_ours.max() if not llm_results_ours.empty else None\n",
    "                \n",
    "                # Compare with original\n",
    "                orig_rnn = original_results[dataset][\"RNN_baseline\"][\"accuracy\"]\n",
    "                orig_llm = original_results[dataset][\"LLM_PEFT_best\"][\"accuracy\"]\n",
    "                \n",
    "                comparison_data.append({\n",
    "                    'Dataset': dataset,\n",
    "                    'Method': 'RNN',\n",
    "                    'Original_Paper': orig_rnn,\n",
    "                    'Our_Replication': our_best_rnn,\n",
    "                    'Difference': (our_best_rnn - orig_rnn) if our_best_rnn else None\n",
    "                })\n",
    "                \n",
    "                comparison_data.append({\n",
    "                    'Dataset': dataset,\n",
    "                    'Method': 'LLM-PEFT',\n",
    "                    'Original_Paper': orig_llm,\n",
    "                    'Our_Replication': our_best_llm,\n",
    "                    'Difference': (our_best_llm - orig_llm) if our_best_llm else None\n",
    "                })\n",
    "    \n",
    "    if comparison_data:\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        print(\"\\n Replication Accuracy Comparison:\")\n",
    "        print(comparison_df.round(4))\n",
    "        \n",
    "        # Save comparison\n",
    "        comparison_df.to_csv(RESULTS_DIR / \"replication_comparison.csv\", index=False)\n",
    "        \n",
    "        # Calculate replication fidelity\n",
    "        valid_comparisons = comparison_df.dropna(subset=['Difference'])\n",
    "        if not valid_comparisons.empty:\n",
    "            mean_diff = valid_comparisons['Difference'].mean()\n",
    "            std_diff = valid_comparisons['Difference'].std()\n",
    "            \n",
    "            print(f\"\\n Replication Fidelity:\")\n",
    "            print(f\"  Mean difference: {mean_diff:.4f} ± {std_diff:.4f}\")\n",
    "            print(f\"  Close replication (±0.05): {(abs(valid_comparisons['Difference']) <= 0.05).sum()}/{len(valid_comparisons)}\")\n",
    "    \n",
    "else:\n",
    "    print(\" No replication results available for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate key findings summary\n",
    "print(\"\\n Key Findings Summary\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "findings = []\n",
    "\n",
    "if not results_df.empty and len(results_df[results_df['status'] == 'success']) > 0:\n",
    "    successful_df = results_df[results_df['status'] == 'success'].copy()\n",
    "    \n",
    "    # Overall success rate\n",
    "    success_rate = len(successful_df) / len(results_df) * 100\n",
    "    findings.append(f\"Experiment success rate: {success_rate:.1f}% ({len(successful_df)}/{len(results_df)})\")\n",
    "    \n",
    "    # Performance comparison\n",
    "    if 'next_activity_accuracy' in successful_df.columns:\n",
    "        exp_performance = successful_df.groupby('experiment_type')['next_activity_accuracy'].agg(['mean', 'max']).round(4)\n",
    "        \n",
    "        for exp_type in exp_performance.index:\n",
    "            findings.append(f\"{exp_type} - Mean accuracy: {exp_performance.loc[exp_type, 'mean']:.4f}, Best: {exp_performance.loc[exp_type, 'max']:.4f}\")\n",
    "    \n",
    "    # Best performing configurations\n",
    "    if 'next_activity_accuracy' in successful_df.columns:\n",
    "        overall_best = successful_df.loc[successful_df['next_activity_accuracy'].idxmax()]\n",
    "        findings.append(f\"Best overall performance: {overall_best['configuration']} ({overall_best['next_activity_accuracy']:.4f} accuracy on {overall_best['dataset']})\")\n",
    "    \n",
    "    # PEFT strategy insights\n",
    "    if 'peft_strategy' in successful_df.columns:\n",
    "        llm_df = successful_df[successful_df['experiment_type'] == 'LLM-PEFT']\n",
    "        if not llm_df.empty and 'next_activity_accuracy' in llm_df.columns:\n",
    "            best_strategy = llm_df.groupby('peft_strategy')['next_activity_accuracy'].mean().idxmax()\n",
    "            best_strategy_score = llm_df.groupby('peft_strategy')['next_activity_accuracy'].mean().max()\n",
    "            findings.append(f\"Best PEFT strategy: {best_strategy} (mean accuracy: {best_strategy_score:.4f})\")\n",
    "    \n",
    "    # Training efficiency\n",
    "    if 'training_time' in successful_df.columns:\n",
    "        efficiency = successful_df.groupby('experiment_type')['training_time'].mean().round(1)\n",
    "        findings.append(f\"Training time comparison: {dict(efficiency)}\")\n",
    "    \n",
    "    # Dataset insights\n",
    "    dataset_performance = successful_df.groupby('dataset')['next_activity_accuracy'].max().round(4)\n",
    "    easiest_dataset = dataset_performance.idxmax()\n",
    "    hardest_dataset = dataset_performance.idxmin()\n",
    "    findings.append(f\"Dataset difficulty: Easiest - {easiest_dataset} ({dataset_performance.max():.4f}), Hardest - {hardest_dataset} ({dataset_performance.min():.4f})\")\n",
    "\n",
    "else:\n",
    "    findings.append(\"No successful experiments to analyze\")\n",
    "\n",
    "# Print findings\n",
    "for i, finding in enumerate(findings, 1):\n",
    "    print(f\"{i}. {finding}\")\n",
    "\n",
    "# Save findings to file\n",
    "findings_file = RESULTS_DIR / \"key_findings.txt\"\n",
    "with open(findings_file, 'w') as f:\n",
    "    f.write(\"LLM-PEFT-PPM Replication Study - Key Findings\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "    for i, finding in enumerate(findings, 1):\n",
    "        f.write(f\"{i}. {finding}\\n\")\n",
    "\n",
    "print(f\"\\n Key findings saved to: {findings_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replication result files generated and saved in `replication_results/` directory.\n",
    "<!-- ## 8. Files Generated\n",
    "\n",
    "This analysis notebook has generated the following files in `replication_results/`:\n",
    "\n",
    "### 📊 Data Files:\n",
    "- `detailed_results.csv` - Complete results with all metrics\n",
    "- `next_activity_performance.csv` - Next activity prediction performance summary\n",
    "- `remaining_time_performance.csv` - Remaining time prediction performance summary\n",
    "- `replication_comparison.csv` - Comparison with original paper results\n",
    "- `key_findings.txt` - Summary of key findings\n",
    "\n",
    "### 📈 Visualizations:\n",
    "- `accuracy_comparison_boxplot.png` - Performance comparison by method\n",
    "- `peft_strategy_comparison.png` - PEFT strategy effectiveness\n",
    "- `training_time_comparison.png` - Training efficiency comparison\n",
    "- `performance_vs_runtime.png` - Performance vs computational cost\n",
    "- `performance_heatmap.png` - Dataset-method performance matrix\n",
    "\n",
    "### 🔬 Research Questions Addressed:\n",
    "\n",
    "**RQ1: Do PEFT-adapted LLMs outperform traditional approaches?**\n",
    "- Statistical comparison between RNN and LLM-PEFT methods\n",
    "- Performance metrics across all datasets\n",
    "\n",
    "**RQ2: Multi-task vs single-task learning effectiveness?**\n",
    "- Analysis of multi-task configurations (requires additional experiment data)\n",
    "\n",
    "**RQ3: Optimal PEFT strategy identification?**\n",
    "- Comparison of LoRA vs layer freezing strategies\n",
    "- Best performing configurations per dataset\n",
    "\n",
    "### 📝 Next Steps:\n",
    "1. Update original paper comparison values with actual results\n",
    "2. Conduct deeper analysis on specific PEFT configurations\n",
    "3. Analyze Traffic Fines dataset results (when available)\n",
    "4. Create final replication report with conclusions\n",
    "\n",
    "---\n",
    "\n",
    "**Analysis Complete!** 🎉\n",
    "\n",
    "Use these results to write your replication report and compare findings with the original Oyamada et al. paper. -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
