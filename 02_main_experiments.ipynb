{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-PEFT-PPM Experiments - Main Execution Notebook\n",
    "\n",
    "This notebook executes the core experiments for the replication study, including:\n",
    "- RNN baselines with hyperparameter search\n",
    "- LLM-PEFT experiments with different strategies\n",
    "- Multi-task vs single-task comparisons\n",
    "- Extension with Traffic Fines dataset\n",
    "\n",
    "**Note**: This notebook is designed to run on TU/e HPC with GPU access.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Project configuration\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if not (PROJECT_ROOT / \"next_event_prediction.py\").exists():\n",
    "    PROJECT_ROOT = Path.cwd() / \"llm-peft-ppm-replication\"\n",
    "    os.chdir(PROJECT_ROOT)\n",
    "\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Working directory: {PROJECT_ROOT}\")\n",
    "print(f\"Timestamp: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration based on methodology\n",
    "DATASETS = {\n",
    "    \"BPI12\": \"BPI12\",\n",
    "    \"BPI17\": \"BPI17\", \n",
    "    \"BPI20RfP\": \"BPI20RequestForPayment\",\n",
    "    \"BPI20PTC\": \"BPI20PrepaidTravelCosts\",\n",
    "    \"BPI20PD\": \"BPI20PermitData\"\n",
    "    # \"BPITrafficFines\": \"BPITrafficFines\"  # Add when available\n",
    "}\n",
    "\n",
    "# RNN Hyperparameter grid (as specified in methodology)\n",
    "RNN_HYPERPARAMS = {\n",
    "    \"embedding_size\": [32, 128, 256, 512],\n",
    "    \"hidden_size\": [128, 256, 512],\n",
    "    \"lr\": [5e-4, 1e-4, 5e-5],\n",
    "    \"batch_size\": [32, 64, 256],\n",
    "    \"num_layers\": [1, 2, 3, 4, 5, 6]\n",
    "}\n",
    "\n",
    "# LLM configurations\n",
    "LLM_CONFIGS = {\n",
    "    \"qwen25-05b\": {\"embedding_size\": 896, \"hidden_size\": 896, \"params\": \"0.5B\"},\n",
    "    \"llama32-1b\": {\"embedding_size\": 2048, \"hidden_size\": 2048, \"params\": \"1B\"},\n",
    "    \"pm-gpt2\": {\"embedding_size\": 768, \"hidden_size\": 768, \"params\": \"0.1B\"}\n",
    "}\n",
    "\n",
    "# PEFT strategies\n",
    "PEFT_STRATEGIES = {\n",
    "    \"lora\": {\"fine_tuning\": \"lora\", \"r\": 256, \"lora_alpha\": 512},\n",
    "    \"freeze_all\": {\"fine_tuning\": \"freeze\", \"freeze_layers\": \"all\"},\n",
    "    \"freeze_0\": {\"fine_tuning\": \"freeze\", \"freeze_layers\": \"0\"},\n",
    "    \"freeze_0_1\": {\"fine_tuning\": \"freeze\", \"freeze_layers\": \"0,1\"},\n",
    "    \"freeze_last\": {\"fine_tuning\": \"freeze\", \"freeze_layers\": \"-1\"},\n",
    "    \"freeze_last_2\": {\"fine_tuning\": \"freeze\", \"freeze_layers\": \"-1,-2\"}\n",
    "}\n",
    "\n",
    "print(\"Experiment configuration loaded:\")\n",
    "print(f\"  - Datasets: {len(DATASETS)}\")\n",
    "print(f\"  - LLM models: {len(LLM_CONFIGS)}\")\n",
    "print(f\"  - PEFT strategies: {len(PEFT_STRATEGIES)}\")\n",
    "print(f\"  - RNN hyperparameter combinations: {np.prod(list(len(v) for v in RNN_HYPERPARAMS.values()))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(cmd_args, experiment_name, timeout_hours=6):\n",
    "    \"\"\"\n",
    "    Run a single experiment with proper logging and error handling.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüöÄ Starting experiment: {experiment_name}\")\n",
    "    print(f\"Command: python {' '.join(cmd_args)}\")\n",
    "    \n",
    "    # Create log directory\n",
    "    log_dir = PROJECT_ROOT / \"replication_results\" / \"logs\"\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Log files\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_file = log_dir / f\"{experiment_name}_{timestamp}.log\"\n",
    "    error_file = log_dir / f\"{experiment_name}_{timestamp}.err\"\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Run the experiment\n",
    "        with open(log_file, \"w\") as log_f, open(error_file, \"w\") as err_f:\n",
    "            process = subprocess.Popen(\n",
    "                [\"python\"] + cmd_args,\n",
    "                stdout=log_f,\n",
    "                stderr=err_f,\n",
    "                cwd=PROJECT_ROOT\n",
    "            )\n",
    "            \n",
    "            # Wait with timeout\n",
    "            try:\n",
    "                return_code = process.wait(timeout=timeout_hours * 3600)\n",
    "            except subprocess.TimeoutExpired:\n",
    "                process.kill()\n",
    "                return {\"status\": \"timeout\", \"runtime\": timeout_hours * 3600}\n",
    "        \n",
    "        runtime = time.time() - start_time\n",
    "        \n",
    "        if return_code == 0:\n",
    "            print(f\"‚úÖ Completed successfully in {runtime:.1f}s\")\n",
    "            return {\"status\": \"success\", \"runtime\": runtime, \"log_file\": str(log_file)}\n",
    "        else:\n",
    "            print(f\"‚ùå Failed with return code {return_code}\")\n",
    "            # Show last few lines of error log\n",
    "            if error_file.exists():\n",
    "                with open(error_file, \"r\") as f:\n",
    "                    error_lines = f.readlines()[-10:]\n",
    "                    if error_lines:\n",
    "                        print(\"Last error lines:\")\n",
    "                        for line in error_lines:\n",
    "                            print(f\"  {line.strip()}\")\n",
    "            return {\"status\": \"failed\", \"runtime\": runtime, \"error_file\": str(error_file)}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception occurred: {e}\")\n",
    "        return {\"status\": \"exception\", \"error\": str(e)}\n",
    "\n",
    "\n",
    "def save_experiment_results(results, filename=\"experiment_results.json\"):\n",
    "    \"\"\"\n",
    "    Save experiment results to JSON file.\n",
    "    \"\"\"\n",
    "    results_file = PROJECT_ROOT / \"replication_results\" / filename\n",
    "    with open(results_file, \"w\") as f:\n",
    "        json.dump(results, f, indent=2, default=str)\n",
    "    print(f\"üìä Results saved to: {results_file}\")\n",
    "\n",
    "\n",
    "def load_experiment_results(filename=\"experiment_results.json\"):\n",
    "    \"\"\"\n",
    "    Load experiment results from JSON file.\n",
    "    \"\"\"\n",
    "    results_file = PROJECT_ROOT / \"replication_results\" / filename\n",
    "    if results_file.exists():\n",
    "        with open(results_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "\n",
    "print(\"‚úÖ Utility functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quick Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test run with smallest configuration to ensure everything works\n",
    "print(\"üß™ Running quick test to verify setup...\")\n",
    "\n",
    "test_args = [\n",
    "    \"next_event_prediction.py\",\n",
    "    \"--dataset\", \"BPI20PrepaidTravelCosts\",\n",
    "    \"--backbone\", \"qwen25-05b\", \n",
    "    \"--embedding_size\", \"896\",\n",
    "    \"--hidden_size\", \"896\",\n",
    "    \"--lr\", \"0.00005\",\n",
    "    \"--batch_size\", \"4\",\n",
    "    \"--epochs\", \"1\",\n",
    "    \"--categorical_features\", \"activity\",\n",
    "    \"--continuous_features\", \"all\",\n",
    "    \"--categorical_targets\", \"activity\", \n",
    "    \"--continuous_targets\", \"remaining_time\",\n",
    "    \"--fine_tuning\", \"lora\",\n",
    "    \"--r\", \"2\",\n",
    "    \"--lora_alpha\", \"4\"\n",
    "]\n",
    "\n",
    "test_result = run_experiment(test_args, \"quick_test\", timeout_hours=0.5)\n",
    "\n",
    "if test_result[\"status\"] == \"success\":\n",
    "    print(\"\\nüéâ Quick test PASSED! Ready to run full experiments.\")\n",
    "    RUN_FULL_EXPERIMENTS = True\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Quick test FAILED: {test_result}\")\n",
    "    print(\"Please fix issues before running full experiments.\")\n",
    "    RUN_FULL_EXPERIMENTS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RNN Baseline Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN baseline experiments with hyperparameter search\n",
    "if RUN_FULL_EXPERIMENTS:\n",
    "    print(\"üî¨ Starting RNN baseline experiments...\")\n",
    "    \n",
    "    rnn_results = {}\n",
    "    \n",
    "    # For each dataset, run a subset of hyperparameter combinations\n",
    "    # (Full grid search would be too expensive for notebook execution)\n",
    "    \n",
    "    # Reduced hyperparameter grid for notebook execution\n",
    "    reduced_rnn_params = {\n",
    "        \"embedding_size\": [32, 128],\n",
    "        \"hidden_size\": [128, 256],\n",
    "        \"lr\": [1e-4, 5e-5],\n",
    "        \"batch_size\": [64],\n",
    "        \"num_layers\": [2, 4]\n",
    "    }\n",
    "    \n",
    "    for dataset_name, dataset_key in DATASETS.items():\n",
    "        print(f\"\\nüìä Dataset: {dataset_name}\")\n",
    "        rnn_results[dataset_name] = {}\n",
    "        \n",
    "        # Multi-task RNN\n",
    "        experiment_count = 0\n",
    "        for emb_size in reduced_rnn_params[\"embedding_size\"]:\n",
    "            for hidden_size in reduced_rnn_params[\"hidden_size\"]:\n",
    "                for lr in reduced_rnn_params[\"lr\"]:\n",
    "                    for batch_size in reduced_rnn_params[\"batch_size\"]:\n",
    "                        for num_layers in reduced_rnn_params[\"num_layers\"]:\n",
    "                            \n",
    "                            experiment_count += 1\n",
    "                            if experiment_count > 4:  # Limit to 4 combinations per dataset\n",
    "                                break\n",
    "                            \n",
    "                            config_name = f\"rnn_emb{emb_size}_hid{hidden_size}_lr{lr:.0e}_bs{batch_size}_layers{num_layers}\"\n",
    "                            \n",
    "                            args = [\n",
    "                                \"next_event_prediction.py\",\n",
    "                                \"--dataset\", dataset_key,\n",
    "                                \"--backbone\", \"rnn\",\n",
    "                                \"--embedding_size\", str(emb_size),\n",
    "                                \"--hidden_size\", str(hidden_size),\n",
    "                                \"--lr\", str(lr),\n",
    "                                \"--batch_size\", str(batch_size),\n",
    "                                \"--epochs\", \"25\",\n",
    "                                \"--num_layers\", str(num_layers),\n",
    "                                \"--categorical_features\", \"activity\",\n",
    "                                \"--continuous_features\", \"all\",\n",
    "                                \"--categorical_targets\", \"activity\",\n",
    "                                \"--continuous_targets\", \"remaining_time\"\n",
    "                            ]\n",
    "                            \n",
    "                            result = run_experiment(args, f\"{dataset_name}_{config_name}\", timeout_hours=3)\n",
    "                            rnn_results[dataset_name][config_name] = result\n",
    "                            \n",
    "                            # Save intermediate results\n",
    "                            save_experiment_results(rnn_results, \"rnn_baseline_results.json\")\n",
    "                            \n",
    "                            if experiment_count >= 4:\n",
    "                                break\n",
    "                        if experiment_count >= 4:\n",
    "                            break\n",
    "                    if experiment_count >= 4:\n",
    "                        break\n",
    "                if experiment_count >= 4:\n",
    "                    break\n",
    "            if experiment_count >= 4:\n",
    "                break\n",
    "    \n",
    "    print(\"\\n‚úÖ RNN baseline experiments completed!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping RNN experiments due to test failure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LLM-PEFT Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-PEFT experiments\n",
    "if RUN_FULL_EXPERIMENTS:\n",
    "    print(\"ü§ñ Starting LLM-PEFT experiments...\")\n",
    "    \n",
    "    llm_results = {}\n",
    "    \n",
    "    for dataset_name, dataset_key in DATASETS.items():\n",
    "        print(f\"\\nüìä Dataset: {dataset_name}\")\n",
    "        llm_results[dataset_name] = {}\n",
    "        \n",
    "        for model_name, model_config in LLM_CONFIGS.items():\n",
    "            print(f\"  üîß Model: {model_name}\")\n",
    "            llm_results[dataset_name][model_name] = {}\n",
    "            \n",
    "            for strategy_name, strategy_config in PEFT_STRATEGIES.items():\n",
    "                print(f\"    üìã Strategy: {strategy_name}\")\n",
    "                \n",
    "                experiment_name = f\"{dataset_name}_{model_name}_{strategy_name}\"\n",
    "                \n",
    "                # Build command arguments\n",
    "                args = [\n",
    "                    \"next_event_prediction.py\",\n",
    "                    \"--dataset\", dataset_key,\n",
    "                    \"--backbone\", model_name,\n",
    "                    \"--embedding_size\", str(model_config[\"embedding_size\"]),\n",
    "                    \"--hidden_size\", str(model_config[\"hidden_size\"]),\n",
    "                    \"--lr\", \"0.00005\",\n",
    "                    \"--batch_size\", \"64\",\n",
    "                    \"--epochs\", \"10\",\n",
    "                    \"--categorical_features\", \"activity\",\n",
    "                    \"--continuous_features\", \"all\",\n",
    "                    \"--categorical_targets\", \"activity\",\n",
    "                    \"--continuous_targets\", \"remaining_time\"\n",
    "                ]\n",
    "                \n",
    "                # Add PEFT-specific arguments\n",
    "                if strategy_config[\"fine_tuning\"] == \"lora\":\n",
    "                    args.extend([\n",
    "                        \"--fine_tuning\", \"lora\",\n",
    "                        \"--r\", str(strategy_config[\"r\"]),\n",
    "                        \"--lora_alpha\", str(strategy_config[\"lora_alpha\"])\n",
    "                    ])\n",
    "                elif strategy_config[\"fine_tuning\"] == \"freeze\":\n",
    "                    args.extend([\n",
    "                        \"--fine_tuning\", \"freeze\",\n",
    "                        \"--freeze_layers\", strategy_config[\"freeze_layers\"]\n",
    "                    ])\n",
    "                \n",
    "                # Run experiment\n",
    "                result = run_experiment(args, experiment_name, timeout_hours=8)\n",
    "                llm_results[dataset_name][model_name][strategy_name] = result\n",
    "                \n",
    "                # Save intermediate results\n",
    "                save_experiment_results(llm_results, \"llm_peft_results.json\")\n",
    "                \n",
    "                # Short break between experiments\n",
    "                time.sleep(5)\n",
    "    \n",
    "    print(\"\\n‚úÖ LLM-PEFT experiments completed!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping LLM-PEFT experiments due to test failure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Competitor Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competitor baseline experiments\n",
    "if RUN_FULL_EXPERIMENTS:\n",
    "    print(\"üèÜ Starting competitor baseline experiments...\")\n",
    "    \n",
    "    competitor_results = {}\n",
    "    \n",
    "    for dataset_name, dataset_key in DATASETS.items():\n",
    "        print(f\"\\nüìä Dataset: {dataset_name}\")\n",
    "        competitor_results[dataset_name] = {}\n",
    "        \n",
    "        # S-NAP (Narrative-style) baseline\n",
    "        print(\"  üìù Running S-NAP baseline...\")\n",
    "        snap_args = [\n",
    "            \"rebmann_et_al.py\",\n",
    "            \"--dataset\", dataset_key,\n",
    "            \"--epochs\", \"10\"\n",
    "        ]\n",
    "        \n",
    "        snap_result = run_experiment(snap_args, f\"{dataset_name}_snap\", timeout_hours=6)\n",
    "        competitor_results[dataset_name][\"snap\"] = snap_result\n",
    "        \n",
    "        # Transfer Learning baseline\n",
    "        print(\"  üîÑ Running Transfer Learning baseline...\")\n",
    "        transfer_args = [\n",
    "            \"luijken_transfer_learning.py\",\n",
    "            \"--dataset\", dataset_key,\n",
    "            \"--epochs\", \"10\"\n",
    "        ]\n",
    "        \n",
    "        transfer_result = run_experiment(transfer_args, f\"{dataset_name}_transfer\", timeout_hours=6)\n",
    "        competitor_results[dataset_name][\"transfer_learning\"] = transfer_result\n",
    "        \n",
    "        # Save intermediate results\n",
    "        save_experiment_results(competitor_results, \"competitor_baseline_results.json\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Competitor baseline experiments completed!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping competitor experiments due to test failure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and summarize all results\n",
    "print(\"üìä Experiment Results Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load results from JSON files\n",
    "try:\n",
    "    rnn_results = load_experiment_results(\"rnn_baseline_results.json\")\n",
    "    llm_results = load_experiment_results(\"llm_peft_results.json\")\n",
    "    competitor_results = load_experiment_results(\"competitor_baseline_results.json\")\n",
    "    \n",
    "    # Count successful experiments\n",
    "    def count_successful_experiments(results):\n",
    "        count = 0\n",
    "        total = 0\n",
    "        for dataset in results.values():\n",
    "            if isinstance(dataset, dict):\n",
    "                for model_or_config in dataset.values():\n",
    "                    if isinstance(model_or_config, dict):\n",
    "                        if \"status\" in model_or_config:  # Direct result\n",
    "                            total += 1\n",
    "                            if model_or_config[\"status\"] == \"success\":\n",
    "                                count += 1\n",
    "                        else:  # Nested results\n",
    "                            for result in model_or_config.values():\n",
    "                                if isinstance(result, dict) and \"status\" in result:\n",
    "                                    total += 1\n",
    "                                    if result[\"status\"] == \"success\":\n",
    "                                        count += 1\n",
    "        return count, total\n",
    "    \n",
    "    # RNN results\n",
    "    rnn_success, rnn_total = count_successful_experiments(rnn_results)\n",
    "    print(f\"RNN Baselines: {rnn_success}/{rnn_total} successful\")\n",
    "    \n",
    "    # LLM results  \n",
    "    llm_success, llm_total = count_successful_experiments(llm_results)\n",
    "    print(f\"LLM-PEFT: {llm_success}/{llm_total} successful\")\n",
    "    \n",
    "    # Competitor results\n",
    "    comp_success, comp_total = count_successful_experiments(competitor_results)\n",
    "    print(f\"Competitors: {comp_success}/{comp_total} successful\")\n",
    "    \n",
    "    print(f\"\\nOverall: {rnn_success + llm_success + comp_success}/{rnn_total + llm_total + comp_total} experiments successful\")\n",
    "    \n",
    "    # Create summary dataframe\n",
    "    summary_data = []\n",
    "    \n",
    "    # Add successful experiments to summary\n",
    "    for exp_type, results in [(\"RNN\", rnn_results), (\"LLM-PEFT\", llm_results), (\"Competitor\", competitor_results)]:\n",
    "        for dataset_name, dataset_results in results.items():\n",
    "            if isinstance(dataset_results, dict):\n",
    "                for config_name, config_results in dataset_results.items():\n",
    "                    if isinstance(config_results, dict):\n",
    "                        if \"status\" in config_results:\n",
    "                            summary_data.append({\n",
    "                                \"Type\": exp_type,\n",
    "                                \"Dataset\": dataset_name,\n",
    "                                \"Configuration\": config_name,\n",
    "                                \"Status\": config_results[\"status\"],\n",
    "                                \"Runtime (s)\": config_results.get(\"runtime\", 0)\n",
    "                            })\n",
    "                        else:\n",
    "                            for strategy_name, strategy_results in config_results.items():\n",
    "                                if isinstance(strategy_results, dict) and \"status\" in strategy_results:\n",
    "                                    summary_data.append({\n",
    "                                        \"Type\": exp_type,\n",
    "                                        \"Dataset\": dataset_name,\n",
    "                                        \"Configuration\": f\"{config_name}_{strategy_name}\",\n",
    "                                        \"Status\": strategy_results[\"status\"],\n",
    "                                        \"Runtime (s)\": strategy_results.get(\"runtime\", 0)\n",
    "                                    })\n",
    "    \n",
    "    if summary_data:\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Save summary\n",
    "        summary_file = PROJECT_ROOT / \"replication_results\" / \"experiment_summary.csv\"\n",
    "        summary_df.to_csv(summary_file, index=False)\n",
    "        print(f\"\\nüìÑ Summary saved to: {summary_file}\")\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(\"\\nüìà Summary Statistics:\")\n",
    "        print(summary_df.groupby([\"Type\", \"Status\"]).size().unstack(fill_value=0))\n",
    "        \n",
    "        # Runtime statistics for successful experiments\n",
    "        successful_df = summary_df[summary_df[\"Status\"] == \"success\"]\n",
    "        if not successful_df.empty:\n",
    "            print(\"\\n‚è±Ô∏è Runtime Statistics (successful experiments):\")\n",
    "            runtime_stats = successful_df.groupby(\"Type\")[\"Runtime (s)\"].agg([\"mean\", \"std\", \"min\", \"max\"])\n",
    "            print(runtime_stats.round(1))\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ö†Ô∏è Results files not found: {e}\")\n",
    "    print(\"Run the experiments first to generate results.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading results: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "### After completing the experiments:\n",
    "\n",
    "1. **Results Analysis** (`05_results_analysis.ipynb`)\n",
    "   - Load and parse model outputs\n",
    "   - Calculate performance metrics (accuracy, MSE)\n",
    "   - Statistical significance testing\n",
    "   - Performance comparison tables\n",
    "\n",
    "2. **Visualization** (`06_visualization.ipynb`)\n",
    "   - Create performance comparison plots\n",
    "   - Learning curves and convergence analysis\n",
    "   - PEFT strategy comparison\n",
    "\n",
    "3. **Replication Report** \n",
    "   - Document findings vs original paper\n",
    "   - Discuss similarities and differences\n",
    "   - Extension results with Traffic Fines dataset\n",
    "\n",
    "### Files Generated:\n",
    "- `replication_results/experiment_summary.csv`\n",
    "- `replication_results/rnn_baseline_results.json`\n",
    "- `replication_results/llm_peft_results.json`\n",
    "- `replication_results/competitor_baseline_results.json`\n",
    "- `replication_results/logs/` (detailed experiment logs)\n",
    "\n",
    "### Tips for Running:\n",
    "- Monitor GPU memory usage during experiments\n",
    "- Check log files if experiments fail\n",
    "- Adjust batch sizes if running into memory issues\n",
    "- Consider running experiments in smaller batches if needed\n",
    "\n",
    "Good luck with your replication study! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
